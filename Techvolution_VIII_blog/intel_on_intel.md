
# POST ONE

## Dear Intel Team

## I'm thankful

I want to thank the people at Intel. To the over one hundred thousand support staff, technicians, programmers, engineers, chemists, and physicists who work there: your job designing and manufacturing personal computer and server microchips is amazing and way too under appreciated lately.

Just in the past few weeks your media coverage has been "Is Intel Actually Screwed", "COLLAPSE: Intel is Falling Apart", "The End Times of Intel", "Intel Will Never Recover".

The dramatic proclamations against you come from people who want faster computers every year, but aren't in the kitchen dealing with: countless suppliers, waiting on building permits, interpreting environmental rules, following union laws...

All while working on a modern Manhattan Project.

Let's look back at your progress. 

The first commercial transistors were made in the 1950s and were the size of buttons. Nobody, not even physicists and chemists, understood why some worked and some didn't. But you took on the challenge to do better and started and sustained Moore's Law; the 1965 observation that the number of transistors in an integrated circuit doubles about every two years.

Adding more transistors means a more powerful computer chip. And so it's good that your microchip processors went from:

- 4,500 transistors in the early 1970s with the 8080 chip
- 3.2 million transistors in the early 1990s Pentium chip.
- Tens of billions of transistors in the forthcoming Panther Lake processor.

<!-- IMAGE OF INTEL's MICROPROCESSOR HISTORY -->

Few other companies have contributed to our economy at such an incredible rate. Despite the progress, the mystery of semiconductors never went away. The difficulty morphed into trying to comprehend the enormous complexity of the manufacturing processes.

Few people can.

Today, it takes three months for a modern microchip to go from a silicon wafer into a finished product. All the while the microchip goes nearly non-stop through hundreds of very specialized tools. Operating these tools require skill sets so unique their rarity and preciousness is comparable to an endangered species. 

The equipment needed to produce today's chips has become astonishingly specialized. Just think about the printing process of the lithography machines used to print the pattern of transistors on the silicon wafer. Such machines currently cost over $400 million each and produce details smaller than the human DNA molecule.

<!-- IMAGE OF LITHOGRAPHY MACHINE -->

In order to continue the doubling curve required by Moore's Law, Intel and the competition have investigated more than fifty different ways of making chips. Nine out of ten end up in a dead end following an investment of billions of dollars. This industry is perpetually at the edge of physics and economics, where the consequences of one incorrect technical decision can mean lost years or bankruptcy.

In this world of endless experimentation at the boundaries of physical feasibility, the Intel fabrication ability found itself lagging the competition towards the end of 2015. This deficit, in all probability, was approximately 6-12 months in transistor density and speeds that were imperceptible to the average computer user.

Yet in a field that measures “progress” as doubling the performance each two years, the slightest setback becomes a crisis. In the media, the story shouted failure. In the financial world, the dollars dried up. The company that had been at the forefront of “Moore's Law” for the past four decades suddenly became a laggard—not because they failed to continue their innovation curve but because they were innovating at a slightly slower rate than anticipated.

In this fog of development, Intel's ability to pack transistors onto microchips fell behind others.

As media reporters are apt to say, the other guys make faster microchips with more transistors, perhaps being 6-12 months ahead (although the gap should close with the impending "18A and 14A process nodes" I'm told).

And because of this misstep, you at Intel got the scorn for failing to meet the expectations of huge yearly improvements you helped create.

## Still in the Race

Of course Intel wasn't always behind. Its workers lead the Moore's Law race for many decades. Your founder Dr. Gordon Moore interwove scientific discovery and innovation into the company's DNA. And then he gave the reins to Dr. Andy Grove who instituted accountability according to his famous maxim "only the paranoid survive".

Thus, you had a long history of designing and making the best microchips. From the late 1980s business and customers were effectively forced to buy from you.

This commercial success challenged your commitment to innovation. You sometimes threw your weight around by stomping competition or over pressuring suppliers for better deals. You even felt confident enough to reject Steve Jobs when he asked you to make the microchips for Apple's new device, the iPhone.

Then in 2015, your ability to manufacture the fastest microprocessors faltered.

It is my understanding that when the Low Numerical Aperture Extreme Ultraviolet lithography tool wasn't ready, you tried cobalt interconnects and extensive multi-patterning instead (and I'm sure countless other techniques too complex for my enthusiast brain).

It didn't work out. You got stuck in the mystery of semiconductor manufacturing and went from being years ahead to years behind in the race to add more transistors to microchips.

<!-- PHOTO MASKS IMAGE -->

By this time, most other semiconductor companies closed or sold their manufacturer fabrication departments (called "fabs"). It had become too expensive and difficult to stay in the Moore's Law race. Instead, Apple, AMD, nVidia, and others embraced being "fabless" and sent their designs to a semiconductor foundry to fabricate. 

Effectively that meant one company, Taiwan Semiconductor Manufacturing Company (TSMC). When Intel's problems continued for years after 2015, TSMC overtook you, and now had both the fastest microchips and the scale of making most of the world's chips.

When you tried to get other customers into your own foundry business in 2021, few had (technical) reasons to join up (although interest has been picking up lately). From 1990 to 2009 USA share of advanced manufacturing dropped from 40% to about 10%. And most of that 10% was the often ridiculed Intel's slower microchips.

So Intel was at the rear of the pack in the race of Moore's Law.

So what.

Ferrari hasn't won a F1 title in generations. The Chicago Cubs took over a century to win a championship. NASA couldn't come up with reuseable rockets.

Yet we still hope the best for these organizations, and give their people money, awards, and accolades because we understand how hard it is to achieve their goals.

Your naysayers and press followers won't do that for you. They should.

It's not that you are perfect. I'm sure those on the inside know more than anyone else the hubris, missteps, and mistakes that saw a titan become a napping rabbit.

I mean you were founded by PhDs and probably shouldn't have anyone in control who does not see the Periodic Table as a recipe book (it was a business minded CEO who rejected making the iPhones chips). I hear there are too many middle managers there (like in much of the American economy). Also, share buybacks are a bad idea in an industry as unpredictable and capital intensive as yours.

But you can still put billions of transistors on a microchip. So I just wanted to say thanks for going to work every day.

In the next blog post, we'll uncover why more people should be thankful too.

# POST TWO

## Why "Intel" Matters, Part 2.

## What Are Not Biologists for Humans

You can know the name of a bird in all the languages of the world, but when you're finished, you'll know absolutely nothing whatever about the bird... So let's look at the bird and see what it's doing—that's what counts. I learned very early the difference between knowing the name of something and knowing something.

_Richard P. Feynman, "What Do You Care What Other People Think?": Further Adventures of a Curious Character_

In a prior post we discussed two concepts, biologists for humans, and how Moore's Law is causing our political and economic discord by birthing ever more powerful tools that we don't yet know how to use properly.

We can flesh out these two concepts using the case of American semiconductor manufacturing.

The first is showcasing how a "biologist for humans" thinks about Intel.

I'm not here to defend a billion dollar corporation such as Intel, or the other American based semiconductor manufacturers such as Texas Instruments or Wolfspeed. I am here to defend the people who work together at a place we call "Intel".

And the distinction there is the difference Feynman talked about.

The company of Intel has struggled. From the mid 1980s to the mid 2010s, it made many of the world's advanced microprocessors, and then missed out on making mobile smartphone chips and then AI chips while losing half its huge market share of the lucrative home computer and service chips to competitors.

Hence the ridicule of tech and investor press and the population at large.

And yet, I doubt most computer users would have any idea what microchips are in their computers or servers, and would hardly notice the 1-10% difference in performance from using Intel's locally manufactured microchips.

It's important to remember that during its troubles Intel was always getting better, just not at the rate society had grown accustomed to.

Those labeling Intel as a loser, were effectively labeling the workers as 'losers', and thereby dismissing the only group of people in our society manufacturing the microchips we need—all over performance differences most people would never notice, especially since Intel overcame its problems solving a very hard problem, within five years.

In short media, investor, and public observers had a list of expectations of semiconductor manufacturing derived from human created narratives of Moore's Law, such as computer gamers desire for more "frames per a second", or investors expected quarterly revenue, and because these storylines were interrupted, blamed the people at Intel for their error instead of assessing the technological ecosystem and its health. 

We need not be so crass. Here's how we could do better. Let's see how a biologist responds to problems in the natural world.

## What Are Biologists

> "Chestnuts roasting on an open fire, Jack Frost nipping at your nose."

_The Christmas Song by Mel Torme, Robert Wells_

For thousands of years, the American Chestnut was more than just a tree—it was a vital part of the ecosystem in eastern North America. Its large and nutritious nuts supported a wide range of wildlife, from deer and blue jays to pigs and humans. The tree's wood was also highly valued for its durability and versatility, making it a key material for things like homes, furniture, and railroads. For all these reasons the American Chestnut played a central role in the region's ecology, economy, and culture.

Until a fungus blight came in the late 1800s.

Unable to combat it, about four billion American chestnut trees died in a few decades. Scientists think perhaps under a dozen survived that can still reproduce. This functional extinction is a profound disruption; forests once filled with creatures dining on endless chestnuts are much quieter today; Americans must roast European or Asian chestnuts over open fires.

What did biologists do?

They responded to the American Chestnut crisis by systematically investigating the problem.

Even in the relatively primitive early 1900s, biologists identified Cryphonectria parasitica as the fungal pathogen causing widespread tree death, tracing its origin to imported Asian chestnuts. Early researchers developed multiple strategies to combat the disease, including cross-breeding with blight-resistant Chinese Chestnut varieties, genetic engineering, and biological control methods.

The primary goal was to develop chestnut trees that could survive the fungal infection and reproduce. Modern efforts use new tools such as genetic modification to introduce resistance genes directly into the American Chestnut's DNA.

Progress has been slow but society understood and followed what biologists told them the situation was.

Hence we could take action.

Today government and non-profits are breeding and planting blight resistant American chestnut saplings throughout America and Canada. It'll take many years to see it again throughout the forests, but progress is being made, and fingers crossed, we are on the way returning the species and its huge benefits back into the environment.

Biologists, media, the public, did this because they did not see the "bird", i.e. the human narrative we give to describe events which so easily makes us inactive, incurious, and allows us to cast blame on natural events.

Instead, biologists analyzed what the tree was doing, therefore saw the problem, investigated it, and implemented a solution.

In comparison, when we see American semiconductor manufacturing suffering, we saw the problem from within the confines of human narratives.

When gamers complained about gaming PCs not as fast as they wanted, or investors their portfolios not as large as they expected, it was "Intel's problem", as if the product of approx 100,000 people's using thousands of extremely specialized tools to do extremely advanced work, need not be understood, analyzed, investigated, aided, and because knowing all these details is hard even for people inside the organization, perhaps given some patience.

It's comparable to our ancestors who might have seen the many wilting American Chestnut, and because it failed to provide the expected nutritious nuts, blamed a person, a group of people, or perhaps the gods for the failure.

Instead, we should have seen a sickness inside the technological ecosystem that was causing the delay in Moore's Law improvements, and responded as biologists do. 

Because just as biologists study ecosystems—understanding how species interact, adapt, and sometimes struggle within their environment—we need to approach our technological world with the same systematic thinking. 

Intel's challenges aren't isolated corporate failures; they're symptoms within a complex technological ecosystem where thousands of companies, workers, suppliers, archaic systems and recent innovations all interconnect. When one part of this system faces stress, society would wait for biologists to investigate the underlying causes rather than simply assigning blame based on personally held expectations.

This is what thinking like a "biologist for humans" means: seeing technological problems as ecosystem issues requiring systematic diagnosis rather than blame. In the final post, we'll explore what this perspective reveals about American semiconductor manufacturing, and why the implications extend far beyond one industry.

# POST THREE

## Why "Intel" Matters, Part 3.

## What Are Biologists For Humans

Globally we produce more than a trillion computer chips per year—each containing anywhere from thousands to tens of billions of transistors. In total, humanity now manufactures more transistors annually than there are stars in the observable universe.

For perspective: the Apollo 11 spacecraft that landed us on the Moon in 1969 had about 40,000 transistors total. A single modern smartphone contains over 10 billion—250,000 times more computing power in your pocket than the technology that got us to the moon.

This exponential growth represents the power of Moore's Law, and even with occasional missteps, it is changing our politics, economics, identity, and environment much faster than our biology evolved to understand. This mismatch requires us to think like "biologists for humans"—people who study our technological ecosystem with the same systematic approach biologists use in nature.

A biologist for humans realizes modern technology is no longer a collection of products, but a living ecosystem we depend on and co-evolve with.

<!-- Moon landing -->

Like biological species, each tool must continuously adapt, harvest resources, and evolve to survive inside its ecosystem. Therefore, just as a biologist studies an organism's behaviors, environmental interactions, and reproductive health (see American Chestnut), we must study technology as a dynamic, interconnected system constantly reshaping the human experience.

Then we'd know that when computer gamers, investors, customers, and others criticize the people at Intel for making microchips a few percentage points slower than others, it drastically misses the point.

Because if Americans and the tools of its ecosystem are struggling making the transistors we desperately need, we shouldn't mock the trees for making less chestnuts each year.

Instead, we should identify and rectify the blight.

Because like in nature, there will be good and bad harvest years, but extinction is never returned from. Therefore, making much faster microprocessors each year is not nearly as important as simply continuing to make advanced transistors.

Following Feynman's advice, we should replace human narratives focused on Intel's people with an understanding of what's actually happening in our technological ecosystem. And then tech reviewers, investors, and all of us should investigate the technological environment and identify systemic problems.

In a prior post we noted the huge increase in health care administrators and administration in the American economy since 1970.

A similar large increase has been noted in permitting requirements, environmental assessments, government grant wait times, overly long worker safety training, frivolous patent proliferation, and so much more making it hard for all American manufacturers, including semiconductor manufacturers, to get their work done and continually repopulate the technological forest.

It's beyond the scope of this blog to investigate these impediments, but for a quick reference here is a quote from Walter Issacson's book _The Innovators_, "In 2011 a milestone was reached: Apple and Google spent more on lawsuits and payments involving patents than they did on research and development of new products."

With all the power of digital technology to make paperwork, it has ironically created a sort of thicket of administration that it's strangling its own technological ecosystem to death.

And while foreign made microchips are often amazing and can certainly help during bad "transistor harvest" years in America, it doesn't solve the problem. And not just for the supply chain resilience and geo-political concerns often cited.

The biggest reason is seen inside simple evolution, the same reason biologists can't just plant European or Asian Chestnuts.

As Steve Jobs said, "The best way to predict the future is to invent it". There is no other way to the future than to make it. You can't buy it, or even design it, because that assumes your desires or thoughts are perfect.

We need to actually mix everyday problems, thoughts and ideas, and available raw materials. As Dr. Gordon Moore and Dr. Andy Grove proved, innovation needs survival instincts to keep life alive. Only going through that process reveals what is needed and possible with the resources at hand.

That's why the first computers were imagined from the textile looms of 1830s England (the world's most advanced), or why Henry Ford's team imagined car mass manufacturing after looking at a butchery in Chicago (again among the world's most advanced). Even Jobs had to see the graphical user interface (GUI) at Xerox PARC in order to invent the Macintosh, the first modern personal computer.

Absence of our society making new tools goes a long way to explaining our economic troubles.

Evolution solves problems via constant iterations. New lifeforms venture out, and whichever procreate most prove themselves best adapted to the changing environment. As we lose the ability to fabricate transistors, we also lose our ability to imagine and create the new tools we need to solve our everyday problems that keep getting worse.

We've started to value manufacturing again, but not nearly enough yet to recover from our office job glut. Motorola, IBM, Global Foundries were all stellar American semiconductor manufacturers, now mostly gone under the same rancor Intel is now getting. The infamous COVID related chip shortage showcases how important chips are to us. Yet by 2009, U.S. employment in the semiconductor manufacturing industry was lower than in the early 1990s, prior to the personal computing boom.

For all these reasons, we must cherish the people at "Intel".

While we are still learning how to best use them, they invented and planted the technological ecosystem of the next age such as the microprocessor, USB, PCIe, and countless other gadgets, gizmos, and tools. These workers face endless hours turning raw materials into our digital lifeblood and therefore deserve our respect and thanks.

If their current company isn't worth having around, as is sometimes thought, then start a new one. Japan started a semiconductor manufacturing company called Rapidus and it appears to be doing alright so far.

Narratives will always serve a crucial purpose—they help coordinate massive human efforts toward shared goals. NASA's moon landing succeeded partly because society grasped the inspiring story even when technical details remained out of reach. The humans volunteering to plant American Chestnuts are inspired to save the tree, bring back Christmas Chestnut roasting, and in general reverse a wrong.

But narratives become problematic when they replace systematic understanding or create unrealistic expectations that harm the very ecosystems they're meant to advance. Terms like "this laptop is falling behind" and "that company has a quarterly earnings beat" are narrative shortcuts we need, but that also prevent us from seeing the gradual, interconnected changes actually reshaping our technological landscape.

All to our own detriment in our investments and personal lives.

Even today, when Intel releases a quality product or earnings report, the narrative changes quickly. But that again is still over-interpreting technological events through human narratives of product review and investor expectations, click rates and return rates, which Feynman told us is misleading. Because once the "crop" of products falls behind expectations, it's back to neglect instead of investigation and assistance, as a biologist would provide.

If this sort of enlightenment sounds like wishful thinking, think of this, as it stands the USA is going to have spent many billions birthing the Internet Age products and technology, and is getting close to have no way of supplying even its own microchips. And that's scary. Because like in biology, once a skill set is gone, it goes extinct. And the rest of the ecosystem that co-evolved with it will inevitably suffer as has North American forests that once thrived on chestnuts.

Truly understanding our role as "biologists for humans" requires a new paradigm. One that recognizes we're not just technology users, but participants in a technological ecosystem that demands the same careful stewardship we give to natural environments.

Let's talk about that.

## Techvolution

In a previous blog post, we noted how the more transistors are created, the worse political bipartisanship, government debt, and other important metrics get.

So what's with the rush to keep adding transistors as fast as possible, even if it means we lose our ability to even make them?

The blight on our microchip manufacturing is not from a foreign country, as it was for the unfortunate American Chestnut, but a foreign time period. We are using biological instincts and ideological beliefs from a pre-digital technological age that still thinks technology is scarce and thus must be mass-produced as quickly as possible.

To understand ourselves in the Internet Age, we need a new paradigm. One that puts technology at the forefront of our identities, politics, and economics.

Until we change our mindsets, we'll go on using increasingly powerful technology with outdated biological instincts. 

But recognizing this mismatch also points toward a solution. 

What if we applied evolutionary principles to understand how civilizations adapt to new technological environments? The same forces that have guided life for billions of years might show us how to thrive in our digital age instead of being torn apart by it. 

I've been exploring this evolutionary framework for our technological civilization, and the early findings are promising. If you're curious about this approach to our modern struggles, you can read more at books.techvolution.org. 

This blog will continue examining how we might consciously evolve beyond our current political and social breakdown. Because if transistors helped create these problems, understanding technological evolution might help us solve them.


# P.S.

- does it matter? what are we racing against that overcomes geopolitical, supply chain, and proximity? yes i know there is expertise but getting the iPhone 23 a year after the current timeline is a change likely to shut down our economy...

- it would require a book to fully flesh the message out (especially for anyone who believes in "comparative advantage"). We don't think of our technological ecosystem as something needing study, management, and care. But we still need it, so we act out using emotions instead of reason....

- it exposes what may be the fatal flaw of western liberal democracies. you can't win and your ecosystem lose. meaning you're victory must be a local material one, and not because of legal ownership of a far away place...no lion can be a king because of a foriegn jungle.

- you can't cheer for a brand like we do "teams" in a sporting event, because teams are organized and everyone always gets another chance. brands are really infrastructure and everyone should be cheering, and supporting, their bridges, buildings, and manufacturers to succeed. Otherwise, you're just assuming someone else will do your work for you and not take advantage of you.

- USA is going to spend billions on AI and this and that, and end up with not a single way of making a chip.

 - industry folk, you're competing yourself to death. literally, you're exported away. why is waiting and being a little more patient for the next node such a big deal...?



## Sources

<!-- POST ONE -->

Gamers Nexus. (2025, August). COLLAPSE: Intel is Falling Apart [Video]. https://www.youtube.com/watch?v=cXVQVbAFh6I

LMG Clips. (2025, August). The End Times of Intel [Video]. https://www.youtube.com/watch?v=4ADwOm7Z3aI

Linus Tech Tips. (2025, August). Intel Will Never Recover From This...-Threadripper 9980X [Video]. https://www.youtube.com/watch?v=HE1ueYjPCOQ

Moore's law. In Wikipedia. Retrieved October 14, 2025, https://en.wikipedia.org/wiki/Moore%27s_law

List of Intel processors. In Wikipedia. Retrieved October 14, 2025, https://en.wikipedia.org/wiki/List_of_Intel_processors

Kapoor, R., & Adner, R. (2007, November). The evolution of lithography. Solid State Technology. https://faculty.wharton.upenn.edu/wp-content/uploads/2012/06/TechnologyInterdependence.pdf

Shilov, Anton. (2024, April 17). ASML sets density record with latest chipmaking tools — High-NA EUV equipment prints first patterns. https://www.tomshardware.com/tech-industry/asml-sets-density-record-with-latest-chipmaking-tools-high-na-euv-equipment-prints-first-patterns

Morales, Jowi. (2024, November 3). TSMC rumored to receive High NA EUV machines from ASML this year. https://www.tomshardware.com/tech-industry/semiconductors/tsmc-rumored-to-receive-high-na-euv-machines-from-asml-this-year

Asianometry. (2023). The History of the Semiconductor Photomask [Video]. https://www.youtube.com/watch?v=Pt9NEnWmyMo

Shilov, Anton (2024, October 2). Corning's Extreme ULE glass debuts for next-gen High-NA EUV chipmaking. https://www.tomshardware.com/tech-industry/corning-unveils-extreme-ule-glass-for-use-in-next-generation-microchip-production

Dwarkesh Patel. (2024, October 2). Asianometry & Dylan Patel — How the semiconductor industry actually works [Video]. https://youtu.be/pE3KKUKXcTM?si=sYALWECw5rDna10o&t=2409

Asianometry. 2023 August 20. What Goes On Inside a Semiconductor Wafer Fab [Video].  https://www.youtube.com/watch?v=p5JQX1BvsDI


<!-- POST TWO -->

Rich, B. R. (1994). Skunk Works: A personal memoir of my years at Lockheed. Back Bay Books.

McLeod, J. (2025, September 9). Rapidus, IBM, and the billion-dollar silicon sovereignty bet. SemiWiki. https://semiwiki.com/semiconductor-manufacturers/361523-rapidus-ibm-and-the-billion-dollar-silicon-sovereignty-bet/

ThePrimeTime. (2025, August). Why Performance Actually Matters (The Standup) [Video] https://www.youtube.com/watch?v=RlTVMi4JzZA&list=PL2Fq-K0QdOQiJpufsnhEd1z3xOv2JMHuk&index=9

Fairchild Semiconductor. (2023). In Wikipedia. https://en.wikipedia.org/wiki/Fairchild_Semiconductor#Fairchildren_companies

TED-Ed. (2025, February 2025). How are microchips made? - George Zaidan and Sajan Saini. https://www.youtube.com/watch?v=IkRXpFIRUl4.

LaurieWired. (2025, October). what if humans forgot how to make CPUs? https://www.youtube.com/watch?v=L2OJFqs8bUk

McLeod, J. (2025, September 9). Rapidus, IBM, and the billion-dollar silicon sovereignty bet. SemiWiki. https://semiwiki.com/semiconductor-manufacturers/361523-rapidus-ibm-and-the-billion-dollar-silicon-sovereignty-bet/

<!-- POST THREE -->

Office of the White House. (2024, March 20). U.S. Semiconductor Jobs are Making a Comeback. https://bidenwhitehouse.archives.gov/cea/written-materials/2024/03/20/u-s-semiconductor-jobs-are-making-a-comeback/


## Article names
COLLAPSE: Intel is Falling Apart
The End Times of Intel
Intel Will Never Recover From This... - Threadripper 9980X
The Fall of Intel

## Ignorance of ecosystems

Start by noting the devasation to ecologies/biosystems from human actions, with little regard to long term effects, ala the American buffalo or European lion et al. Then note how tech YouTubers, et al are dismissing intel for being Ferrari of F1...

## Why so serious?

Since its emergence, semiconductor lithography has witnessed no fewer than 11 distinct attempts to introduce new tool technologies (see table). Tlie intnxluction of these tools has enahled many advances in the semiconductor industry. While the forward progress of the industry is beyond question, this progress has been punctuated by numerous false starts and cases of unrealized potential.

## History of Lithography Machines

| Technology generation | First industry sale | Initial res. (um)  | Market dominance |
|-----------------------|---------------------|--------------------|------------------|
| Contact               | 1962                | 7                  | n/a              |
| Proximity             | 1972                | 3                  | 1973             |
| Projection            | 1973                | 2                  | 1977             |
| E-beam                | 1976                | 0.5                | Never            |
| X-ray                 | 1978                | 0.3                | Never            |
| G-line                | 1978                | 1.25               | 1982             |
| I-line                | 1985                | 0.8                | 1991             |
| DUV 248nm             | 1986                | 0.45               | 1998             |
| DUV 193nm             | 1996                | 0.15               | 2006             |
| DUV 157nm             | 1998                | 0.1                | Never            |
| DUV 193nm immersion   | 2005                | 0.04               | Early Days       |

source: <https://faculty.wharton.upenn.edu/wp-content/uploads/2012/06/TechnologyInterdependence.pdf>

## Fair children

https://en.wikipedia.org/wiki/Fairchild_Semiconductor#Fairchildren_companies

## Chip History

8008
Introduced April 1, 1972
3,500 transistors at 10 μm

8080
Introduced April 1, 1974
4,500 transistors at 6 μm

80386DX
Introduced October 17, 1985
275,000 transistors at 1 μm

80486DX
Introduced April 10, 1989
1.2 million transistors at 1 μm; the 50 MHz was at 0.8 μm

Original Pentium
Introduced March 22, 1993
3.2 million transistors

Pentium with MMX Technology
Introduced January 8, 1997
4.5 million transistors

Pentium III
Introduced February 26, 1999
Katmai – 0.25 μm process technology
9.5 million transistors

Pentium II Xeon and Pentium III Xeon
400 MHz introduced June 29, 1998
9.5 million transistors at 0.25 μm or 28 million at 0.18 μm

Intel Core 2
Introduced July 27, 2006
Conroe – 65 nm process technology
291 million transistors

Core i5 (2nd and 3rd generation)
Introduced January, 2011
Sandy Bridge (Core i5 2nd generation) – 32 nm process technology
995 million transistors

## Lithography machines

### Intro

Semiconductor lithography machines are critical tools used in the manufacturing of semiconductor devices, especially microchips. Their main purpose is to transfer intricate circuit patterns onto silicon wafers, which form the foundational structures of integrated circuits used in electronics like smartphones and computers.

The process works by projecting light through a photomask or reticle, which contains the blueprint of the circuit pattern. This pattern is then reduced and precisely focused onto a silicon wafer coated with a photosensitive material called photoresist. The exposure alters the photoresist in specific areas according to the pattern, allowing subsequent steps to etch the actual circuit onto the wafer.

There are several types of lithography machines based on the light source used:

Optical lithography (photolithography) uses ultraviolet (UV) light for pattern projection, widely used for larger feature sizes.

Extreme Ultraviolet (EUV) lithography employs much shorter wavelength light (13 nm) to create finer, smaller, and more complex patterns necessary for advanced microchips.

Electron beam lithography, which uses focused electron beams rather than light, offers extremely high precision mainly for research and prototyping but is slower for mass production.

Key components of these machines include the light source (UV, EUV, or electron beam), photomasks/reticles for patterning, and wafer stages to hold and precisely position the silicon wafer during exposure.

These lithography machines enable the production of smaller, faster, and more efficient microchips by allowing very fine, high-density transistor layouts. They are fundamental in pushing forward electronic device performance, supporting AI, IoT, and other advanced technologies by fabricating complex semiconductor devices.

In essence, semiconductor lithography equipment is the cornerstone technology that drives the creation of the micro-scale electrical patterns necessary to build integrated circuits powering modern electronics.

https://www.perplexity.ai/search/what-are-semiconductor-lithogr-lCIgm8guTWGd_T3pr4gN8A

### Cost 

Present day litho tools cost ~$50-$80 million. But in a high volume manufacturing
(HVM) environment, such as a chip making factory, they operate 24 hours a day, 7 days a week, 365 days a year, at > 90 % up time, generally for at least 5 years. A state-of-theart 193 nm wavelength litho tool requires only about 20 seconds to 30 seconds to completely expose a 300 mm diameter silicon wafer. This corresponds to a “throughput” of ~120 to ~180 wafers per hour. Thus, during its 5 year lifetime, a tool will expose on the order of 5 million wafers which amortizes the $50-80 million tool cost to only about $10-$16 per full-wafer exposure. A fully patterned 300 mm diameter wafer will yield a few hundred to a few thousand microchips depending on the chip size. For a chip requiring 30 exposures to complete it, the tool cost contributes from only a few cents to a few dollars to the cost of each chip. 

https://appliedmathsolutions.com/pdf/Common_Misconceptions_about_Photolithography_141009.pdf

### Development History

Notable make and models of semiconductor lithography machines during their development, especially from the dominant player ASML, include:

PAS 2000 (1980s): ASML's first system, inherited from Philips and initially sold under the name ASM Lithography. It faced early challenges including oil-based hydraulics unsuitable for cleanrooms and limited commercial success.

PAS 5500 (1990s): Became a widely adopted stepper system by ASML that allowed for smaller chip features with improved alignment and throughput. Many of these older systems remain operational today.

TWINSCAN Series (2001 onward): Introduced dual-stage technology that increased throughput by simultaneously exposing one wafer while aligning the next. This series includes models like:

TWINSCAN AT:1150i (first immersion lithography machine, 2003)

TWINSCAN XT:1700i (first volume immersion production, 2006)

TWINSCAN XT:1900i (industry highest numerical aperture of 1.35, 2007)

TWINSCAN NXT and NXT:1950i (major immersion lithography systems enabling chip feature sizes down to 32 nm)

TWINSCAN NXE Series (EUV lithography, 2010s–present): These machines utilize extreme ultraviolet light (~13.5 nm wavelength) to produce much finer patterns. Notable models include:

NXE:3100 (early commercial EUV)

NXE:3350B

NXE:3600D (best-selling model, extremely complex and costly)

High-NA EUV Systems (Launching 2023-2025): Next-generation EUV lithography machines with higher numerical aperture (0.55 vs. 0.33) for even finer resolution. First shipments went to Intel in 2023 and TSMC in late 2024.

### Estimated Prices

ASML PAS 2000 (1980s): The PAS 2000 was ASML's first system launched in 1984. Early on, the project required about 120 million Dutch guilders (roughly €245 million in today's money) for development and production scale-up. However, the actual sale prices in the early 1980s were far lower per unit, but precise data isn't widely documented.

ASML PAS 5500 (1990s): This machine became wildly successful and helped ASML emerge as a market leader. In the 1990s, lithography tools similar to PAS 5500 were priced around tens of millions of dollars per unit, with a typical estimate for advanced steppers in the $10 million to $30 million range.

ASML TWINSCAN Immersion Systems (2000s): These featured major technological advancements like water immersion to improve feature resolution. Prices for top-end immersion lithography scanners during the 2000s to early 2010s ranged from approximately $30 million to $60 million per system depending on the model and capabilities.

ASML NXE EUV Lithography Systems (2010s–present): EUV machines are among the world's most expensive semiconductor equipment. ASML EUV tools typically cost approximately $150 million to $250 million per system.

ASML Next-Generation High-NA EUV Systems (Launching 2023-2025): These newest lithography machines with higher numerical aperture technology are expected to cost even more, in the range of $300 million or more per unit.

## F1 

https://en.wikipedia.org/wiki/List_of_Formula_One_World_Drivers%27_Champions
https://en.wikipedia.org/wiki/List_of_Formula_One_World_Constructors%27_Champions
https://www.espn.com/f1/standings/_/group/constructors

## Sources

https://www.the-waves.org/2022/12/21/semiconductor-lithography-economics-fuelling-moores-law-and-market-power/

https://faculty.wharton.upenn.edu/wp-content/uploads/2012/06/TechnologyInterdependence.pdf

## Quote

You can know the name of a bird in all the languages of the world, but when you're finished, you'll know absolutely nothing whatever about the bird... So let's look at the bird and see what it's doing—that's what counts. I learned very early the difference between knowing the name of something and knowing something.

_Richard P. Feynman, "What Do You Care What Other People Think?": Further Adventures of a Curious Character_


>My father had taught me, "do you know what that bird is?. It's a brown throated thrush. But in Portuguese it's a... in Italian a..." He says "in Chinese a..., in Japanese a...". Etcetera. He says, now you know in all the languages the name of that bird is and when you've finished with all that. He says "you'll know absolutely nothing whatever about the bird. You only know about humans in different places and what they call the bird. Now he says, "Let's look at the bird. And what it's doing. He doesn't give me a name. He knew the difference between knowing the name of something and knowing something"

https://www.youtube.com/shorts/zWT-nzEAByE